<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Knowledge Ecosystem - Deep Learning and Education</title>
  <meta name="description" content="Lets start with a future view of an individuals education. Many of us have used the internet to educate ourselves with the abundance of medium to high quality videos, papers, articles, podcasts and how-tos being uploaded from numerous individuals, groups, and institutions like never before. Let us imagine that all of what you have learned online, throughout the entirety of your life, from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you’ve read, watched, or listened to, were all added structurally to your education journey and what if that could be consolidated into what we might call a digital education footprint.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Knowledge Ecosystem - Deep Learning and Education" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/dyadxmachina/can-machines-teach" />
  <meta property="og:image" content="https://github.com/dyadxmachina/can-machines-teachimg/MtoQA.png" />
  <meta property="og:description" content="Lets start with a future view of an individuals education. Many of us have used the internet to educate ourselves with the abundance of medium to high quality videos, papers, articles, podcasts and how-tos being uploaded from numerous individuals, groups, and institutions like never before. Let us imagine that all of what you have learned online, throughout the entirety of your life, from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you’ve read, watched, or listened to, were all added structurally to your education journey and what if that could be consolidated into what we might call a digital education footprint." />
  <meta name="github-repo" content="dyadxmachina/can-machines-teach" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Knowledge Ecosystem - Deep Learning and Education" />
  
  <meta name="twitter:description" content="Lets start with a future view of an individuals education. Many of us have used the internet to educate ourselves with the abundance of medium to high quality videos, papers, articles, podcasts and how-tos being uploaded from numerous individuals, groups, and institutions like never before. Let us imagine that all of what you have learned online, throughout the entirety of your life, from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you’ve read, watched, or listened to, were all added structurally to your education journey and what if that could be consolidated into what we might call a digital education footprint." />
  <meta name="twitter:image" content="https://github.com/dyadxmachina/can-machines-teachimg/MtoQA.png" />

<meta name="author" content="Fanli Zheng (Christian) &amp; Haohan Wang">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="concepts.html">
<link rel="next" href="conclusion.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128379860-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-128379860-3');
</script>

<!-- Primary Meta Tags -->
<title>Teaching Machines - Artificial Intelligence and Education </title>
<meta name="title" content="Teaching Machines - Artificial Intelligence and Education ">
<meta name="description" content="Let's start with a future view of an individual's education. Many of us have used the internet to educate ourselves with the abundance of high quality videos, papers, articles, podcasts and how-tos all over the web. Let us imagine that all of what you have learned online (throughout the entirety of your life), from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you've read, watched, or listened to, were all consolidated into what we might call a **digital education footprint**.   The **digital education footprint** would string together our online education into a concrete representation of an individual's online education and could be extended into more formal settings. By showcasing the broad range of individual's knowledge (making digital music) as well as the places they've went deeper than most (deep learning or philosophy of mathematics), we could begin to accept education as a life-long journey rather than one monolithic part of an individual's past. This would begin to show us a more accurate depiction of individual's education that could be updated each and everytime one educates themselves. With every new year, their footprint would evolve just as the very thread of their lives would. Visualised over time, we would be able to see an individual's journey or even a whole communities. Seeing how differen  In this essay, I will propose a new way to approach education which will require significant effort to bring to life but I believe the benefits will surely outweigh the costs. I'll talk about how we can use machine learning and deep learning in particular to help create our **digital education footprint**, **student journeys**, and a collective human knowledge graph. This will allow us to take the space of unstructured educational content and begin to map it unto a knowledge graph and use generative models to make educational content engaging and test a student's knowledge no matter the subject. I will name a few benefits of such a future. ">

<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="https://dyadxmachina.github.io/can-machines-teach/">
<meta property="og:title" content="Teaching Machines - Artificial Intelligence and Education learn the math needed to design artificial neural networks">
<meta property="og:description" content="Let's start with a future view of an individual's education. Many of us have used the internet to educate ourselves with the abundance of high quality videos, papers, articles, podcasts and how-tos all over the web. Let us imagine that all of what you have learned online (throughout the entirety of your life), from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you've read, watched, or listened to, were all consolidated into what we might call a **digital education footprint**.   The **digital education footprint** would string together our online education into a concrete representation of an individual's online education and could be extended into more formal settings. By showcasing the broad range of individual's knowledge (making digital music) as well as the places they've went deeper than most (deep learning or philosophy of mathematics), we could begin to accept education as a life-long journey rather than one monolithic part of an individual's past. This would begin to show us a more accurate depiction of individual's education that could be updated each and everytime one educates themselves. With every new year, their footprint would evolve just as the very thread of their lives would. Visualised over time, we would be able to see an individual's journey or even a whole communities. Seeing how differen  In this essay, I will propose a new way to approach education which will require significant effort to bring to life but I believe the benefits will surely outweigh the costs. I'll talk about how we can use machine learning and deep learning in particular to help create our **digital education footprint**, **student journeys**, and a collective human knowledge graph. This will allow us to take the space of unstructured educational content and begin to map it unto a knowledge graph and use generative models to make educational content engaging and test a student's knowledge no matter the subject. I will name a few benefits of such a future. ">
<meta property="og:image" content="https://dyadxmachina.github.io/can-machines-teach/img/MtoQA.png">


<!-- Primary Meta Tags -->
<title>Teaching Machines - Artificial Intelligence and Education by Haohan Wang and Fanli (Christian) Zheng</title>
<meta name="title" content="Teaching Machines - Artificial Intelligence and Education by Haohan Wang and Fanli (Christian) Zheng">
<meta name="description" content="Let's start with a future view of an individual's education. Many of us have used the internet to educate ourselves with the abundance of high quality videos, papers, articles, podcasts and how-tos all over the web. Let us imagine that all of what you have learned online (throughout the entirety of your life), from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you've read, watched, or listened to, were all consolidated into what we might call a **digital education footprint**.   The **digital education footprint** would string together our online education into a concrete representation of an individual's online education and could be extended into more formal settings. By showcasing the broad range of individual's knowledge (making digital music) as well as the places they've went deeper than most (deep learning or philosophy of mathematics), we could begin to accept education as a life-long journey rather than one monolithic part of an individual's past. This would begin to show us a more accurate depiction of individual's education that could be updated each and everytime one educates themselves. With every new year, their footprint would evolve just as the very thread of their lives would. Visualised over time, we would be able to see an individual's journey or even a whole communities. Seeing how differen  In this essay, I will propose a new way to approach education which will require significant effort to bring to life but I believe the benefits will surely outweigh the costs. I'll talk about how we can use machine learning and deep learning in particular to help create our **digital education footprint**, **student journeys**, and a collective human knowledge graph. This will allow us to take the space of unstructured educational content and begin to map it unto a knowledge graph and use generative models to make educational content engaging and test a student's knowledge no matter the subject. I will name a few benefits of such a future. ">

<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="https://dyadxmachina.github.io/can-machines-teach/">
<meta property="og:title" content="Teaching Machines - Artificial Intelligence and Education by Haohan Wang and Fanli (Christian) Zheng">
<meta property="og:description" content="Let's start with a future view of an individual's education. Many of us have used the internet to educate ourselves with the abundance of high quality videos, papers, articles, podcasts and how-tos all over the web. Let us imagine that all of what you have learned online (throughout the entirety of your life), from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you've read, watched, or listened to, were all consolidated into what we might call a **digital education footprint**.   The **digital education footprint** would string together our online education into a concrete representation of an individual's online education and could be extended into more formal settings. By showcasing the broad range of individual's knowledge (making digital music) as well as the places they've went deeper than most (deep learning or philosophy of mathematics), we could begin to accept education as a life-long journey rather than one monolithic part of an individual's past. This would begin to show us a more accurate depiction of individual's education that could be updated each and everytime one educates themselves. With every new year, their footprint would evolve just as the very thread of their lives would. Visualised over time, we would be able to see an individual's journey or even a whole communities. Seeing how differen  In this essay, I will propose a new way to approach education which will require significant effort to bring to life but I believe the benefits will surely outweigh the costs. I'll talk about how we can use machine learning and deep learning in particular to help create our **digital education footprint**, **student journeys**, and a collective human knowledge graph. This will allow us to take the space of unstructured educational content and begin to map it unto a knowledge graph and use generative models to make educational content engaging and test a student's knowledge no matter the subject. I will name a few benefits of such a future. ">
<meta property="og:image" content="https://dyadxmachina.github.io/can-machines-teach/img/MtoQA.png">

<!-- Twitter -->
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:url" content="https://dyadxmachina.github.io/can-machines-teach/">
<meta property="twitter:title" content="Teaching Machines - Artificial Intelligence and Education by Haohan Wang and Fanli (Christian) Zheng">
<meta property="twitter:description" content="Let's start with a future view of an individual's education. Many of us have used the internet to educate ourselves with the abundance of high quality videos, papers, articles, podcasts and how-tos all over the web. Let us imagine that all of what you have learned online (throughout the entirety of your life), from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you've read, watched, or listened to, were all consolidated into what we might call a **digital education footprint**.   The **digital education footprint** would string together our online education into a concrete representation of an individual's online education and could be extended into more formal settings. By showcasing the broad range of individual's knowledge (making digital music) as well as the places they've went deeper than most (deep learning or philosophy of mathematics), we could begin to accept education as a life-long journey rather than one monolithic part of an individual's past. This would begin to show us a more accurate depiction of individual's education that could be updated each and everytime one educates themselves. With every new year, their footprint would evolve just as the very thread of their lives would. Visualised over time, we would be able to see an individual's journey or even a whole communities. Seeing how differen  In this essay, I will propose a new way to approach education which will require significant effort to bring to life but I believe the benefits will surely outweigh the costs. I'll talk about how we can use machine learning and deep learning in particular to help create our **digital education footprint**, **student journeys**, and a collective human knowledge graph. This will allow us to take the space of unstructured educational content and begin to map it unto a knowledge graph and use generative models to make educational content engaging and test a student's knowledge no matter the subject. I will name a few benefits of such a future. ">
<meta property="twitter:image" content="https://dyadxmachina.github.io/can-machines-teach/img/MtoQA.png">

<!-- Twitter -->
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:url" content="https://dyadxmachina.github.io/can-machines-teach/">
<meta property="twitter:title" content="Teaching Machines - Artificial Intelligence and Education by Haohan Wang and Fanli (Christian) Zheng ">
<meta property="twitter:description" content="Let's start with a future view of an individual's education. Many of us have used the internet to educate ourselves with the abundance of high quality videos, papers, articles, podcasts and how-tos all over the web. Let us imagine that all of what you have learned online (throughout the entirety of your life), from the hundreds of Youtube videos, Wikipedia articles, Nature papers, and podcasts you've read, watched, or listened to, were all consolidated into what we might call a **digital education footprint**.   The **digital education footprint** would string together our online education into a concrete representation of an individual's online education and could be extended into more formal settings. By showcasing the broad range of individual's knowledge (making digital music) as well as the places they've went deeper than most (deep learning or philosophy of mathematics), we could begin to accept education as a life-long journey rather than one monolithic part of an individual's past. This would begin to show us a more accurate depiction of individual's education that could be updated each and everytime one educates themselves. With every new year, their footprint would evolve just as the very thread of their lives would. Visualised over time, we would be able to see an individual's journey or even a whole communities. Seeing how differen  In this essay, I will propose a new way to approach education which will require significant effort to bring to life but I believe the benefits will surely outweigh the costs. I'll talk about how we can use machine learning and deep learning in particular to help create our **digital education footprint**, **student journeys**, and a collective human knowledge graph. This will allow us to take the space of unstructured educational content and begin to map it unto a knowledge graph and use generative models to make educational content engaging and test a student's knowledge no matter the subject. I will name a few benefits of such a future. ">
<meta property="twitter:image" content="https://dyadxmachina.github.io/can-machines-teach/img/MtoQA.png">



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Knowledge Ecosystem: Deep Learning and Education</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="primary-concerns.html"><a href="primary-concerns.html"><i class="fa fa-check"></i><b>2</b> Primary Concerns</a><ul>
<li class="chapter" data-level="2.1" data-path="primary-concerns.html"><a href="primary-concerns.html#passive-consumption-and-untested-knowledge"><i class="fa fa-check"></i><b>2.1</b> Passive Consumption and Untested Knowledge</a></li>
<li class="chapter" data-level="2.2" data-path="primary-concerns.html"><a href="primary-concerns.html#knowledge-ecosystem-example"><i class="fa fa-check"></i><b>2.2</b> Knowledge Ecosystem Example</a></li>
<li class="chapter" data-level="2.3" data-path="primary-concerns.html"><a href="primary-concerns.html#the-problem-of-knowledge-representation"><i class="fa fa-check"></i><b>2.3</b> The Problem of Knowledge Representation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="concepts.html"><a href="concepts.html"><i class="fa fa-check"></i><b>3</b> Concepts</a><ul>
<li class="chapter" data-level="3.1" data-path="concepts.html"><a href="concepts.html#knowledge-footprint"><i class="fa fa-check"></i><b>3.1</b> Knowledge Footprint</a></li>
<li class="chapter" data-level="3.2" data-path="concepts.html"><a href="concepts.html#knowledge-journeys"><i class="fa fa-check"></i><b>3.2</b> Knowledge Journeys</a></li>
<li class="chapter" data-level="3.3" data-path="concepts.html"><a href="concepts.html#collective-human-knowledge-graph"><i class="fa fa-check"></i><b>3.3</b> Collective Human Knowledge Graph</a><ul>
<li class="chapter" data-level="3.3.1" data-path="concepts.html"><a href="concepts.html#ec2qa-network"><i class="fa fa-check"></i><b>3.3.1</b> EC2QA Network</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="concepts.html"><a href="concepts.html#knowledge-ecosystem-by-example"><i class="fa fa-check"></i><b>3.4</b> Knowledge Ecosystem by Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html"><i class="fa fa-check"></i><b>4</b> Implementing the Knowledge Ecosystem</a><ul>
<li class="chapter" data-level="4.1" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#problem-formulation"><i class="fa fa-check"></i><b>4.1</b> Problem Formulation</a><ul>
<li class="chapter" data-level="4.1.1" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#before-we-start"><i class="fa fa-check"></i><b>4.1.1</b> Before we start…</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#learning-feedback"><i class="fa fa-check"></i><b>4.2</b> Learning + Feedback</a><ul>
<li class="chapter" data-level="4.2.1" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#question-formulation"><i class="fa fa-check"></i><b>4.2.1</b> Question Formulation</a></li>
<li class="chapter" data-level="4.2.2" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#current-research"><i class="fa fa-check"></i><b>4.2.2</b> Current Research</a></li>
<li class="chapter" data-level="4.2.3" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#question-generation"><i class="fa fa-check"></i><b>4.2.3</b> Question Generation</a></li>
<li class="chapter" data-level="4.2.4" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#question-answering"><i class="fa fa-check"></i><b>4.2.4</b> Question Answering</a></li>
<li class="chapter" data-level="4.2.5" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#summary-of-learning-and-feedback-networks-draft"><i class="fa fa-check"></i><b>4.2.5</b> Summary of Learning and Feedback Networks [DRAFT]</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#knowledge-graph-draft"><i class="fa fa-check"></i><b>4.3</b> Knowledge Graph [DRAFT]</a><ul>
<li class="chapter" data-level="4.3.1" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#what-is-a-graph"><i class="fa fa-check"></i><b>4.3.1</b> What is a graph?</a></li>
<li class="chapter" data-level="4.3.2" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#problem-formulation-2"><i class="fa fa-check"></i><b>4.3.2</b> Problem Formulation</a></li>
<li class="chapter" data-level="4.3.3" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#automatic-knowledge-graph-construction"><i class="fa fa-check"></i><b>4.3.3</b> Automatic Knowledge Graph Construction</a></li>
<li class="chapter" data-level="4.3.4" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#case-studies-5"><i class="fa fa-check"></i><b>4.3.4</b> Case Studies</a></li>
<li class="chapter" data-level="4.3.5" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#summary-of-knowledge-graph-draft"><i class="fa fa-check"></i><b>4.3.5</b> Summary of Knowledge Graph [DRAFT]</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#knowledge-journeys-draft"><i class="fa fa-check"></i><b>4.4</b> Knowledge Journeys [DRAFT]</a><ul>
<li class="chapter" data-level="4.4.1" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#summary-of-current-research-and-needs-draft"><i class="fa fa-check"></i><b>4.4.1</b> Summary of Current Research and Needs [DRAFT]</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#data-and-annotation-draft"><i class="fa fa-check"></i><b>4.5</b> Data and Annotation [DRAFT]</a><ul>
<li class="chapter" data-level="4.5.1" data-path="implementing-the-knowledge-ecosystem.html"><a href="implementing-the-knowledge-ecosystem.html#reference-draft"><i class="fa fa-check"></i><b>4.5.1</b> Reference [DRAFT]</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="next-steps.html"><a href="next-steps.html"><i class="fa fa-check"></i><b>6</b> Next Steps</a><ul>
<li class="chapter" data-level="6.1" data-path="next-steps.html"><a href="next-steps.html#creating-a-research-dataset"><i class="fa fa-check"></i><b>6.1</b> Creating a research dataset</a><ul>
<li class="chapter" data-level="6.1.1" data-path="next-steps.html"><a href="next-steps.html#ec2qa-dataset"><i class="fa fa-check"></i><b>6.1.1</b> EC2QA dataset</a></li>
<li class="chapter" data-level="6.1.2" data-path="next-steps.html"><a href="next-steps.html#pretraining"><i class="fa fa-check"></i><b>6.1.2</b> Pretraining</a></li>
<li class="chapter" data-level="6.1.3" data-path="next-steps.html"><a href="next-steps.html#educator-enrichment"><i class="fa fa-check"></i><b>6.1.3</b> Educator Enrichment</a></li>
<li class="chapter" data-level="6.1.4" data-path="next-steps.html"><a href="next-steps.html#minimum-viable-dataset"><i class="fa fa-check"></i><b>6.1.4</b> Minimum Viable Dataset</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="next-steps.html"><a href="next-steps.html#call-for-collaborators"><i class="fa fa-check"></i><b>6.2</b> Call for collaborators</a></li>
<li class="chapter" data-level="6.3" data-path="next-steps.html"><a href="next-steps.html#conclusion-1"><i class="fa fa-check"></i><b>6.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="about-authors.html"><a href="about-authors.html"><i class="fa fa-check"></i><b>7</b> About Authors</a><ul>
<li class="chapter" data-level="7.1" data-path="about-authors.html"><a href="about-authors.html#haohan-wang"><i class="fa fa-check"></i><b>7.1</b> Haohan Wang</a></li>
<li class="chapter" data-level="7.2" data-path="about-authors.html"><a href="about-authors.html#fanli-zheng-christian-ramsey"><i class="fa fa-check"></i><b>7.2</b> Fanli Zheng (Christian Ramsey)</a></li>
<li class="chapter" data-level="7.3" data-path="about-authors.html"><a href="about-authors.html#contact"><i class="fa fa-check"></i><b>7.3</b> Contact</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>8</b> References</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/dyadxmachina/can-machines-teach" target="blank">Github</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Knowledge Ecosystem - Deep Learning and Education</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="implementing-the-knowledge-ecosystem" class="section level1">
<h1><span class="header-section-number">4</span> Implementing the Knowledge Ecosystem</h1>
<p>In this section, we are set to solve the following question:</p>
<blockquote>
<p>How might we approach designing such a knowledge ecosystem?</p>
</blockquote>
<p>We will walk you through some possible implementations of the proposed knowledge ecosystem. We will be presenting the current research in machine learning that is relevant to each element of the knowledge ecosystem while aslo discussing a new artificial neural network architecture EC2QA and what would be needed to design such a network</p>
<p>Note that this is not a blueprint that has to be put together, but instead, each of these components can be developed independently and vary from what you find here. This is a provocation for getting started on the knowledge ecosystem today.</p>
<div id="problem-formulation" class="section level2">
<h2><span class="header-section-number">4.1</span> Problem Formulation</h2>
<p>Building such a knowledge ecosystem is not a trivial task. A necessary step of searching for the proper solutions would be trying to break the whole system into independent elements that we can deal with separately. As what we have discussed earlier, we’ve divided the ecosystem into the elements below:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Learning + Feedback</strong> – given a learner consumes a piece of educational content, reliably evaluate their knowledge and provide the feedback for improvement to support their learning. (credibility, rigour)</p></li>
<li><p><strong>Knowledge Graph</strong> – general knowledge blueprint, as a map to piece together all the content that is currently available. (relatibility, predictability)</p></li>
<li><p><strong>Knowledge Journeys</strong> – given a piece of educational content, classify it within a knowledge graph; given multiple learner journeys, create a way to customize their own growth journey while offereing a way for them them to compare, connect with and follow, other’s journey (compare, traverse, curiosity)</p></li>
<li><p><strong>Knowledge Footprint</strong> – given a learner’s journey, collapse it into a representative symbol(s) (digital education footprint) (relatibility, stable but evolving system )</p></li>
</ol>
<p>Much of the element #1 and #2 have been made possible with the recent breakthoughs in machine learning especially in the filed of deep learning. A few other pieces like the element #3 and #4 may require a more advanced framework that has not been proposed yet to best resolve. Let’s first take a look at some methods that might come handy when applying to our problems.</p>
<p>In this section we will be primarily focus on the first 2 building blocks.</p>
<div id="before-we-start" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Before we start…</h3>
<p>As we discussed above, one of the possible and optimal solutions so far for building our ecosystem is using current state-of-art tehcniques (i.e. deep learning) especially techniques related to education, here I will walk you through recent trends in the form of a survey.</p>
<p>To best illustrate the problem and possible solutions, we could focus on the whole space of different types of educational content ranging from text to podcasts, but we will focus solely on taking educational videos (Youtube lecture or how-to videos). Videos not only happen to be complex but there are also one of the richest media types for knowledge acquisition. Keep in mind that our ultimate goal is to be able to apply our approach to any type of online educational content including open texts, digital texts, audio or podcasts.</p>
<p>Let’s explore some of the solutions that would enable us to design each element of our ecosystem.</p>
</div>
</div>
<div id="learning-feedback" class="section level2">
<h2><span class="header-section-number">4.2</span> Learning + Feedback</h2>
<p>We consider learning + feedback as a key building block of the ecosystem which would settle our concerns to do with passive knowledge consumption and the untested knowledge of learners who watch such lectures. By solving this problem, our ecosystem will be able to provide our learners a credible picture of their tested knowledge and an interactive learning experience that can best support knowledge acuqisition.</p>
<div id="question-formulation" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Question Formulation</h3>
<p>In short, in this section we will be providing some insights on how to solve the following puzzle:</p>
<blockquote>
<p>“how can we take a single educational content and properly test a learner’s knowledge while also providing insightful feedback to support their learning?”.</p>
</blockquote>
</div>
<div id="current-research" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Current Research</h3>
<p>So our learning and feedback component should do the following &gt; Generate a set of questions and answers for any eduational content &gt; Evaluate closed and open ended answers &gt; Provide a score for the content based on the learner’s performance</p>
<p>We will now explore current research trends in deep learning that will help us solve the problem.</p>
<p>In previous years, deep learning research has taken up a similar problem titled Question Generation (QG) and Question Answering (QA).</p>
<p>Question Generation (QG) was originally part of NLP. The goal of QG is to generate questions according to some given information. It could be used in many different scenarios i.e. generating questions for reading comprehension, generating data from large scale question-answering pairs or even generating questions from images. Earlier approaches to QG mainly used human-crafted rules and patterns to transform a descriptive sentence to a related question. Recent neural network-based approaches represent the state-of-art of most of those tasks and this approach has been successfully applied to many other NLP tasks i.e. neural machine translation, summarization, etc. As the training optimization studies progress, the stability and performance improvements are gauranteed.</p>
<p>As for Question Answering (QA) task, it is a well-researched problem in NLP as well. Recently, QA has also been used to develop dialog systems and chatbots designed to simulate human conversation. Traditionally, most of the research used a pipeline of conventional linguistically-based NLP techniques i.e. parsing, part-of-speech tagging and coreference resolution. However, with recent developments in deep learning, neural network models have shown promise for QA. Further improvement i.e.attention mechanism and memory networks allow the network to focus on the most revevant facts such that they achieved state-of-art performance for QA.</p>
<p>Now we have some basic understanding of these 2 problems for which we will be expanding more in depth later. Consider the next question:</p>
<blockquote>
<p>“what types of questions &amp; answers would be best to test a learner’s knowledge given a piece of educational content (i.e. a lecture video)”</p>
</blockquote>
<p>Let’s say a learner is watching a video about hypothesis testing, midway through the video the educator shows an example and provides the data needed to test the hypothesis. It would be ideal if during this time the learner’s knowledge is tested with the following possible questions:</p>
<p>** UPDATE EXAMPLE TO BE MORE TOPIC OR CONCEPT BASED **</p>
<ol style="list-style-type: decimal">
<li><p>What is the definition of the p-value? (and provide multiple choices for learner to choose from)</p></li>
<li><p>Is this a 1-sided test? (answers provided would be: YES or NO)</p></li>
<li><p>How would you intepret the p-value in the context of this example.</p></li>
<li><p>What is the difference beween null hypothesis and alternative hypothesis based on your understanding?</p></li>
</ol>
<p>It is also helpful to ask learner the question as follows after showing the solution:</p>
<ol start="5" style="list-style-type: decimal">
<li>Tell me what you have learned through this video or this example.</li>
</ol>
<p>As shown above, we would call questions #1 and #2 the close-ended questions; question #3 and #4 the specific open-ended questions; and question #5 a general open-ended question.</p>
<p>Based the above information, we can update our question formulation into:</p>
<ol style="list-style-type: decimal">
<li><p>Generate close-ended question + answers pairs</p></li>
<li><p>Generate specific open-ended question + answers pairs</p></li>
<li><p>Evaluate and comment on the general open-ended answers</p></li>
</ol>
<p>In terms of the close-ended, the answers can be well defined and evaluated. However, the process might be a little bit tricky when it comes to the open-ended questions. We will approach each of them here from the current research perspective.</p>
<p>** Why deep learning? **</p>
<p>As we stated above, deep learning has achieved state-of-art performance in both QG and QA tasks. But how?</p>
<p>If you pay close attention to the question generation and answer generation type of problems, you can easily reframe this problem into a general machine learning problem in which the model needs to learn the relationship between the educational content and the meaningful question &amp; answer pairs that is associated with the content. In other words, our problem could be simplified as learning a function that is capable of capture the relationship between our input and output, or, appropriately map the educational content to the desired question and answer pairs with this function.</p>
<p>To the best of our knowledge, deep learning is one of the most optimal techniques currently developed to learn such complex representations of complex data such as video lectures.</p>
<p>By definition, machine learning is subfiled of Artificial Intelligence that uses statistical learning techniques to give the machine the ability to learn from the data. It explores the algorithms that can be used to parse data, learn from the data, and then apply what they have learned to make intelligent decisions. Or more specifically, deep learning is a subset of machine learning that belongs to the family of representation learning. Inside this family, deep learning is particularly good at sampling the features and having additonal layers for more abstract feature learning. All of these features are crutial for our goal of mapping the feature to the output task.</p>
<p>Because of the above advantages, deep learning is known as one of the most flexible machine learning algorithms that can learn and map a <strong>deep representation</strong> of supervised concepts within the data. Deep nueral network architecture can be composed into a single differentiable function and trained end-to-end until it converges. As a result, they can help identify the suitable <em>inductive</em> <em>biases</em> catered to the training data.</p>
<p>Moreover, deep learning outperforms other techniques when the training data is large and the advantage fits our siutation well. We could easily find a large amound of educational content available on the web.</p>
<p>The large amount content creates another problem that can be avoided with deep learning, which is it’s going to be very troublesome if you plan to do feature engineering manually. When there is lack of domain understanding for feature introspection, deep learning is preferable.</p>
<p>In the end, deep learning really shines when it comes to many specialized research problems such as NLP, Visual Recognition and Speech recognition. For solving our task, all those domians will possibly be invloved.</p>
</div>
<div id="question-generation" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Question Generation</h3>
<p>Let’s begin with question generation (QG) problem.</p>
<p>The ideal goal of an automatic question generation is to generate a question Q that is syntactically and semantically correct, relevant to the context and meaningful to answer.</p>
<p>In order to achieve this goal,, we need to train an algorithm to learn the underlying conditional probability distribution</p>
<p><span class="math display">\[P_{\theta}(Q|X)\]</span></p>
<p>parametrized by <span class="math inline">\(\theta\)</span>. In other words, we can think of this problem as the one that requires the model to learn a function (with a set of parameters) <span class="math inline">\(\theta\)</span> during the training stage using content-question and/or answer pairs so that the probability <span class="math inline">\(P_{\theta}(Q|P)\)</span> is maximized over the given training dataset.</p>
<p>It is also helpful to frame this problem into a seq2seq learning problem since both the input and the output are most likely a sequence of text character that the model needs to process and learn the relationship from.</p>
<div id="case-studies" class="section level4">
<h4><span class="header-section-number">4.2.3.1</span> Case Studies</h4>
<ol style="list-style-type: decimal">
<li>In this paper <a href="http://www.princeton.edu/~shitingl/papers/18l@s-qgen.pdf">QG-Net: A Data-Driven Question Generation Model for Educational Content</a>. They use a bi-directional LSTM network to process the input context words sequence. Encoding the answer into context word vectors.</li>
</ol>
<p>QG-Net generates questions by iteratively sampling question words from the conditional probability distribution <span class="math inline">\(P(Q|C,A,\theta)\)</span> where <span class="math inline">\(\theta\)</span> denotes the set of parameters. In order to construct the probability distribution, they first create a <strong>context reader</strong> that process each word <span class="math inline">\(c_j\)</span> in the input context and turns it into a fix-sized representation <span class="math inline">\(h_j\)</span></p>
<p>Then, they used a <strong>question generator</strong> generates the question text word-by-word, given all context word representation and all question words in previous time steps.</p>
<p>As for the quantitative evaluation, they aimed to minimize the difference between the generated question and the true question in the training set during training. Also, they used the standard back-propagation through time with the mini-batch stochastic gradient descent algorithm to learn the model parameters. They employed teacher forcing procedure for training LSTMs. To enhance performance, they also implemented beam search, a greedy but effective approximation to exhausitively search and select the top 25 cancidate output question sentences. The final one would be the one with the lowest negative log likelihood.</p>
<p>The general QG-Net model Architecture is as below:</p>
<div class="figure">
<img src="img/qgnet.png" alt="ma" />
<p class="caption">ma</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>In this summary <a href="http://www.cs.cornell.edu/~xdu/papers/acl17_dsc_poster.pdf">Learning to Ask</a>, they used a sentence- and paragraph-level seq2seq model to read text from the input content and to generate a question about the input sentence.</li>
</ol>
<p>For the second option, we need to encode both sentence and paragraph that sentence belongs to as input, but only attending source sentence hidden states. The performance could be improved with beam search and UNK replacement.</p>
<ol start="3" style="list-style-type: decimal">
<li>In this paper <a href="https://openreview.net/pdf?id=rk3pnae0b">TOPIC-BASED QUESTION GENERATION</a>, they proposed a topic-based question generation algorithm. The algorithm will be able to take in a input sentence, a topic and a question type; then generate a word sequence related to the topic, question type and the input sentence.</li>
</ol>
<p>They are formulating a conditional likelihood objective function to achieve this goal.</p>
<p>Also, in the paper, they proposed a few frameworks that were used to tackle this problem. The first type is seq2seq model. This model typically uses a bidirectional LSTM as the encoder to encode a sentence and a LSTM as the decoder to generate the target question.</p>
<p>The second approach is question pattern prediction and question topic selection algorithms. It takes in an automatically selected phrase Q and fill this phrase into the pattern that was predicted from pre-mined patterns, which is not done with deep learning.</p>
<p>The last approach is multi-source seq2seq learning which aims to integrate information from multiple sources to boost learning.</p>
<ol start="4" style="list-style-type: decimal">
<li>In this paper <a href="https://arxiv.org/pdf/1808.04961.pdf">A Framework for Automatic Question Generation from Text using Deep Reinforcement Learning</a> they proposed a novel way of solving this problem in which they used a reinforcement learning framework that consists of a generator and an evaluator.</li>
</ol>
<p>They refer to the generator as the <span class="math inline">\(agent\)</span> and the <span class="math inline">\(action\)</span> of the agent is to generate the next work in the question. The probability of decoding a word <span class="math inline">\(P_{\theta}(word)\)</span> gives a stochastic policy.</p>
<p>The evaluator will in turn assign a reward for the output sequence predicted using the current policy of the generator. Based on the reward assigned by the evaluator, the generator updates and improves its current policy. The goal in RL-based question generation is to find a policy that can maximize the sum of the expected return at the end of the sequence generated.</p>
</div>
<div id="summary" class="section level4">
<h4><span class="header-section-number">4.2.3.2</span> Summary</h4>
<p>In this QG section, we have discussed 4 algorithms. They provide us a way to frame our problem for which we can apply generative seq2seq model framework. As for our objective function, we are formuating a conditional probability distribution that is conditioned on the provided content (i.e. the video) and answers. Typically, we can use a bi-directional LSTM as the encoder to encode the content and use a LSTM as the decoder to generate the question.</p>
<p>However, as you probabily have noticed that the above examples are focus mainly on processing the text input data instead of videos directly. It demonstrates that more reserch in this new area is needed so as to meet our particular needs.</p>
</div>
</div>
<div id="question-answering" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Question Answering</h3>
<p>Now, let’s move on to our question answering (QA) step. The general goal of a QA model is to predict an answer to the question based on the information found in the passage, given a passage and a question.</p>
<p>Here are the overview of a basic QA model’s implementation <span class="citation">(<span class="citeproc-not-found" data-reference-id="qa_imp"><strong>???</strong></span>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Build representation for the passage and the question separately.</p></li>
<li><p>Incorporate the question information into the passage.</p></li>
<li><p>Get the final representation of the passage by directly matching it against itself</p></li>
<li><p>Generate the answer</p></li>
</ol>
<p>And the typical techniques applied for solving such a problem includ:</p>
<ul>
<li><p>Embedding</p></li>
<li><p>Encoder Decoder</p></li>
<li><p>Attention Mechanism</p></li>
</ul>
<div id="close-ended-questions" class="section level4">
<h4><span class="header-section-number">4.2.4.1</span> Close-ended Questions</h4>
<div id="visual-question-answering-vqa" class="section level5">
<h5><span class="header-section-number">4.2.4.1.1</span> Visual Question Answering (VQA)</h5>
<p>As what we have covered above, most QG problem focuses solely on generating questions but not the answers based on the context.</p>
<p>VQA is a challenging research problem that focuses on providing a natural language answer given any image and any free-form natural language question. As we are managing to handle the video educational content first that is likely to involve language processing and visual recognition tasks, VQA would be a proper start for us. By leveraging this type of algorithm, we enable our system to easily evaluate the answer provided by learners which could in turn automated the whole question + answering + evaluation cycle.</p>
<p>Since we are dealing with visual input, question-guided attention mechanism is a key component for solving this type of task. Started from the attention mechanism that can adaptively learn the most relevant image regions for a given question. Then to stack multiple question-guided attention mechanisms to learn the attention in an iterative way. Also, it is possible to use bilinear features to integrate the visual features from the image spatial grids with question features to predict attention. Considering the questions in natural language may also contain some noise, the co-attention mechanism can jointly learn the attention for both the image and question.</p>
<div id="case-studies-1" class="section level6">
<h6><span class="header-section-number">4.2.4.1.1.1</span> Case Studies</h6>
<ol style="list-style-type: decimal">
<li>In this paper <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yalong_Bai_Deep_Attention_Neural_ECCV_2018_paper.pdf">Deep Attention Neural Tensor Network for Visual Question Answering</a>, they proposed a novel deep attention neural tensor network that can discover the joint correlation over images, questions and answers with tensor-based representation.</li>
</ol>
<p>As for their workflow, they modeled one of the pairwise interaction (i.e. between image and question) by bilinear features, which is further encoded with the third dimension (i.e. answer) to be a triplet using bilinear tensor product. During this step, the model takes in a question + a corresponding image + candidate answers as the input. A CNN (convolutional neural network) a GRU RNN (recurrent neural network) are used for extracting feature vectors and question respectively. Then the representation is passed on as a multi-modal features and integrated by bilinear pooling module. Moreover, they decompose the correlation of triplets by their question and answer types with a slice-wise attention module on tensor to select the most discriminative reasoning process inference.</p>
<p>In the end, they optimize the proposed network by learning a label regression with KL-divergence losses. They claimed that with these techniques, they can enable scalable training and fast convergence over a large number of answer set. During the inference stage, they feed the embeddings of all candicate answer into the network and then select the answer which has the biggest triplet relevance socre as the final answer.</p>
<p>The high-level network architecture is as follows:</p>
<div class="figure">
<img src="img/vqa.png" alt="Deep Attention Neural Tensor Network" />
<p class="caption">Deep Attention Neural Tensor Network</p>
</div>
<ol start="2" style="list-style-type: decimal">
<li>In this paper <a href="https://arxiv.org/pdf/1804.02088.pdf">Question Type Guided Attention in Visual Question Answering</a>, they proposed a model called Question Type-guided Attention (QTA). This model utilizes the information of question type to dynamically balance visual features from both top-down and bottom-up orders.</li>
</ol>
<p>Finally, they propose a multi-task extension that is trained to predict question types from the lexical inputs during training which generalizes the network into applications that lack question type, with a minimal performance loss.</p>
<p>As for their main contribution, they focus on developing an attention mechanism that can exploit high-level semantic information on the question type to guide the visual encoding process.</p>
<p>Specifically, they introduced a novel VQA architecture that can dynamically gate the contribution of ResNet and Faster R-CNN features based on the question type. In turn, it allows them to integrate the information from multiple visual sources and obtain gains across all question types.</p>
</div>
</div>
<div id="video-question-answering" class="section level5">
<h5><span class="header-section-number">4.2.4.1.2</span> Video Question Answering</h5>
<p>The recent advancements that we discussed above in VQA domain have shown some promising implication. In terms of achieving our particular goal, it is also worth mentioning that VQA might be a good start but it is not sufficient yet. To bridge this gap, let’s focus our attention on some video question answering algorithms that have been proposed.</p>
<div id="case-studies-2" class="section level6">
<h6><span class="header-section-number">4.2.4.1.2.1</span> Case Studies</h6>
<ol style="list-style-type: decimal">
<li>In this paper <a href="https://www.ijcai.org/proceedings/2018/0513.pdf">Multi-Turn Video Question Answering via Multi-Stream Hierarchical Attention Context Network</a>, they proposed a hierarchical attention context network for context-aware question understanding by modeling the hierarchically sequential conversation context structure. They also incorporate the multi-step reasoning process fro the multi-stream hierarchical attention context network to enable the progressive joint representation learning of the multi-stream attentional video and context-aware question embedding.</li>
</ol>
<p>The construct their dataset by collecting the conversational video question answering datasets from YouTubeClips and TACoS-MultiLevel in which the first one has 1987 videos and the second daaset has 1303 videos. They invite 5 pairs of crowd-sourcing workers to construct 5 different conversational dialogs. In total, they have collected 37228 video question answering pairs for TACoS-MultiLevel data and 66806 ones for YouTubeClips data.</p>
<ol start="2" style="list-style-type: decimal">
<li>In this paper <a href="https://arxiv.org/pdf/1512.02902.pdf">MovieQA: Understanding Stories in Movies through Question-Answering</a>, they introduced a new dataset called MovieQA dataset that can evaluate automatic story comprehension from both video and text.</li>
</ol>
<p>They collected 408 subtitled movies and obtained their extended summaries in the form of plot synopses(movie summaries that fans write after watching the movie) from Wikipedia. They used plot synopses as a proxy for the movie. They have annotators create both quizzes and answers pairs by referring to the story plot. Time-stamp is also attached with each question.</p>
<p>In the second step of data collection, they used the multiple-choice answers and question collected as the input to show to the annotators. By doing so, annotators can re-formulate the question and answers while doing the sanity check.</p>
</div>
</div>
<div id="summary-1" class="section level5">
<h5><span class="header-section-number">4.2.4.1.3</span> Summary</h5>
<p>By going through the previous examples, we can see that VQA is very particular type of algorithms that is designed to efficiently process image and text input data while making the inference based on the input. Attention is a typical mechanism applied in this type of problems and multiple forms of multipulation on the attention mechanism used in these models have significantly improved the model performance.</p>
<p>Going from VQA to video question answering algorithm, it has been a great leap. The main insight we can directly draw from these video QA papers is that we can follow their steps to collect and annonate our training data by asking crowd-sourcing workers to construct the question and answer pairs. Also, more advanced algorithm like the one described above multi-stream hierarchical attention context network is in need for dealing with video input data in contrast to static pictures.</p>
</div>
<div id="dual-question-answering-model" class="section level5">
<h5><span class="header-section-number">4.2.4.1.4</span> Dual Question-Answering Model</h5>
<p>Both Question Generaion(QG) and Question Answering(QA) are well-defined 2 sets of models that aim to either infer a question or an answer given the counterpart based on the context. However, they are usually explored separately despite of their intrinsic complementary relationship. In our case, a sysem that can take on both roles simultaneously are needed to fully automated learning + feedback process.</p>
<p>There are some algorithms are designed to fulfill both roles.</p>
<div id="case-studies-3" class="section level6">
<h6><span class="header-section-number">4.2.4.1.4.1</span> Case Studies</h6>
<p>1.In this paper <a href="https://arxiv.org/pdf/1809.01997.pdf">Dual Ask-Answer Network for Machine Reading Comprehension</a> they presente a model that can learn question answering and question generation simultaneousely. They tie the network components that playing the similar roles into 2 tasks to transfer cross-task knowledge during training. Then the cross-modal interaction of question, context and answer is captured with a pair of symmetric hierarchical attenion processes.</p>
<p>The high-level architecture of the model is illustrate as below:</p>
<div class="figure">
<img src="img/qgqa.png" alt="Dual Ask-Answer Network 1" />
<p class="caption">Dual Ask-Answer Network 1</p>
</div>
<div class="figure">
<img src="img/daan.png" alt="Dual Ask-Answer Network 2" />
<p class="caption">Dual Ask-Answer Network 2</p>
</div>
<p>In short, the model is composed of embedding layer, encoding layer, attention layer and output layer. The model is fed with a question-context-answer triplet (Q,C,A) and the decoded Q and A from the output layer. Their loss function consists of 2 parts:</p>
<ul>
<li>negative log-likelihood loss</li>
<li>a coverage loss to penalize repetition of the generated text</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>In this paper <a href="https://arxiv.org/pdf/1805.05942.pdf">Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia</a>, they applied their question-answer pair generation system to 10000 top-ranking Wikipedia articles and create over a million question-answer pairs.</li>
</ol>
<p>In their task formulation part, they mentioned that they break this task into 2 sub-tasks:</p>
<ul>
<li>candidate answer extraction</li>
<li>answer-specific question generation</li>
</ul>
<p>To achieve them, they first identify a set of question-worthy candidate answers ans = (A1, A2,…Ai). For each candidate answer Ai, they then aim to generate a question Q - a sequence of tokens y1,y2,…yn - based on the sentence S that contains candidate Ai such that Q asks about an aspect of Ai (of potential interest to a human) and Q might rely on information from sentences that preceeds S in the paragraph. Mathmatically, they compose a function <span class="math display">\[Q = argmax_Q P(Q|S,C)\]</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>In this paper <a href="http://cvboy.com/pdf/publications/cvpr2018_iqan.pdf">Visual Question Generation as Dual Task of Visual Question Answering</a>, they proposed an end-to-end unified model, Invertible Question Answering (iQAN) to introduce question generation as a dual task of question answering to imrpove VQA pefromance.</li>
</ol>
<p>In achieving their goal, they leverage the <strong>dual learning</strong> framework that is proposed in machine translation area initially, which uses A-to-B and B-to-A translation models to form two closed translation loops and let them teach each other through a reinforcement learning process.</p>
<p>In their VQA component, given a question <span class="math inline">\(q\)</span>, an RNN is used for obtaining the embedded feature <strong>q</strong>, and CNN is used to transform the input image <span class="math inline">\(v\)</span> into a feature map. A MUTAN-based attention module is then used to generate a question-aware visual feature <span class="math inline">\(v_q\)</span> from the image and the question. Later, another MUTAN fusion module is used for obtaining the answer feature <span class="math inline">\(a\hat{}\)</span></p>
<ol start="4" style="list-style-type: decimal">
<li>In this paper <a href="https://arxiv.org/pdf/1709.01058.pdf">A Unified Query-based Generative Model for Question Generation and Question Answering</a>, they propose a query-based generative model for solving both tasks. The model follows the classic encoder-decoder framework. The multi-perspective matching encoder that they are implementing is a bi-directional LSTM RNN model that takes a passage and a query as input and perform query understanding by matching it with the passage from multiple perspectives; The decoder is an attention-based LSTM RNN model with copy and coverage mechanism. In the QG task, a question will be generated from the model given the passge and the target answer, whereas in the QA task, the answer will be generated given the question and the passage. They also leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias (a major problem resulted from sequence learning with cross-entropy loss function).They case both QG and QA tasks into one process by firstly matching the input passage against the query, then generating the output based on the matching results.</li>
</ol>
<p>As for the training process, they first pretrain the model with cross-entropy loss and then they fine tune the model parameters with policy-gradient reinforcement learning to alleviate the exposure bias problem. During the policy-gradient reinforcement learning algorithm, they end up adopting a similar sampling strategy as the scheduled sampling strategy for generating the sampled output.</p>
</div>
</div>
<div id="summary-draft" class="section level5">
<h5><span class="header-section-number">4.2.4.1.5</span> Summary [DRAFT]</h5>
<p>As mentioned earlier, QG and QA tasks are intrinsically binded and one cannot find solution for either of them without taking the other party into account. In this section, we have discussed some approaches that many groups of people have taken to help machine learning both tasks simultaneously. Some exciting findings have been presented here. For our problem, it is very motivating to see these progress and learn from their approaches. In sum, the general setup is similar to dual learning framework, we need to tie QG and QA parts of the algorithms together. In the first diagram of the section, we can see that they connect the loss function from both sides of the model which is very similar to the strategy adopted by <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GAN</a> (Generative adversarial network). Some advanced mechsnisms are proposed as well i.e. symmetric hierarchical attenion and policy-gradient reinforcement learning algorithm.</p>
</div>
</div>
<div id="open-ended-question-draft" class="section level4">
<h4><span class="header-section-number">4.2.4.2</span> Open-ended Question [DRAFT]</h4>
<div id="problem-formulation-1" class="section level5">
<h5><span class="header-section-number">4.2.4.2.1</span> Problem Formulation</h5>
<blockquote>
<p>Open-ended questions bring clarity.</p>
</blockquote>
<p>As we mentioned above, the open-ended question could be roughly splitted into 2 categories. One is general oepn-ended questions or a more specific oepn-ended question.</p>
<p>Technically specking, these 2 categories are not that particular distinct since both problems require the system to be able to draw some conclusion based on the context and question provided; the answer is allowed to have a pretty high degree of freedom. In other words, our system should be able to evaluate the answer with relatively flexible rules or standards.</p>
<p>Based our assumptions, we will just combine these 2 problems into 1 here to show some research findings that can possibly support our unified goal.</p>
<p>It may appear unapproachable at the frist glance to teach a system to have answers for or even evaluate this type of problems. Or it is just an indication that we should probably reframe our issue and break it apart into smaller elements. Based on our research, we suggest thiking of this type of issue as a particular type of QA problem; the difference is that after the QA procedure, we need to match and evaluate the answers generated by machine and the learner such that we can provide an adequate feedback.</p>
<p>Here we would like to start with a existing knowledge evaluation system that has been used for grading the essays automatically - Automated essay scoring (AES) which focuses on automatically analyzing the quality of writing and assigning a score to the text. AES systems may rely not only on grammars, but also on more complex features such as semantics, discourse and pragmatics. It has four general types:</p>
<ul>
<li><p>Essay Grade: it is known as the first AES system.</p></li>
<li><p>Inteligent Essay Assessor: it is using Latent Semantic Analysis features</p></li>
<li><p>E-rater: it has been used by the ETS to score essay portion of GMAT</p></li>
<li><p>IntelliMetric: it is devloped and used by the College Board for placement purposes.</p></li>
</ul>
<p>Enough for the introducion, let’s begin reviewing our papers.</p>
<div id="case-studies-4" class="section level6">
<h6><span class="header-section-number">4.2.4.2.1.1</span> Case Studies</h6>
<ol style="list-style-type: decimal">
<li>In this paper <a href="http://aclweb.org/anthology/N18-1024">Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input</a>, they develop a network that can effectively learn connectedness features between sentences and propose a framework for integrating and jointly training the local coherence model with a state-of-art AES.</li>
</ol>
<p>They examine the robustness of the AES model to adversarially crafted input and specifically focus on input related to local coherence; A local coherence model can evaluate the writing based on its ability to rank coherently ordered sequence of sentences higher than their conterparts.</p>
<p>Their models are Local Coherence (LC) model and LSTM AES model. The first model has 2 main parts: sentence representation and clique representation; and he second model is a combined model that does vector concatenation and joint learning.</p>
<ol start="2" style="list-style-type: decimal">
<li>In this paper <a href="https://www.ijcai.org/proceedings/2018/0512.pdf">Open-Ended Long-form Video Question Answering via Adaptive Hierarchical Reinforced Networks</a>, they study the problem of open-ended video question answering from the viewpoint of adaptive hierachical reinforced encoder-decoder network learning. They present the adaptive hierarchical encoder network to learn the joint representation of the long-form video contents according to the question with adaptive video segmentation. They also develop the reinforced decoder network to generate the naural language answer for open-ended video qeustion answering. Meanwhile, they also construct a large-scale dataset for open-ended long-form video QA and validate the effectiveness of the proposed method.</li>
</ol>
<p>The framework of Adaptive Hierarchical Reinforced Networks is are below:</p>
<div class="figure">
<img src="img/oe.png" alt="Open-Ended Long-form Video QA Network" />
<p class="caption">Open-Ended Long-form Video QA Network</p>
</div>
<p>The first part is the hierarchical encoder networks that learn the joint representation of multimodal attentional video and textual question with adaptive video segmentation.</p>
<p>The second part is the reinforced decoder networks that generate the natural language answers for open-ended video question answering.</p>
<ol start="3" style="list-style-type: decimal">
<li>In this paper, <a href="https://arxiv.org/pdf/1805.11752.pdf">Multi-turn Dialogue Response Generation in an Adversarial Learning Framework</a>, they propose an adversarial learning approach that can generate multi-turn dialogue responses. The network framework that they introduce is call <em>hredGAN</em> that is based on conditional GANs. The generator part of the model is a modified hierachical recurrent encoder-decoder network (HRED) and the discriminator is a word-level bi-directional LSTM RNN that shares context and word embedding with the generator.</li>
</ol>
<p>During the inference step, noise sampling is conditioned on the dialogue history and is used to perturb the generator’s latent space for generating possible responses. The final response is the one ranked the best by the discriminator.</p>
<p>In sum, their hredGAN combines both generative and retrieval-based multi-turn dialogue systems to improve the model’s performance. The underlying mechanism is that generator and the discriminator share the context and word embedding and this allows for joint end-to-end training using back-propogation.</p>
</div>
<div id="summary-2" class="section level6">
<h6><span class="header-section-number">4.2.4.2.1.2</span> Summary</h6>
<p>Based on our limited research, we found that it is also achievable for our system to generate the answers for open-ended questions based on the educational video and provide appropriate feedback/rating for the learner. The first paper presents a newly developed AES model that can rate learner’s writing. It also demonstrates a possible way of enhance the AES model by training it with the adversarially crafted input.</p>
<p>In the second paper, we discuss a network that can answer the open-ended questions based on the video and a given question. Their Adaptive Hierarchical Reinforced Networks are composed of hierarchical encoder networks and the reinforced decoder networks. With their framework, we can easily extend the research topic into educational video specifially.</p>
<p>Similar to the last peper, this paper shows that it is possible to generate responses conditioned on the context. By leveraging conditional GAN model framework, their model’s performance is significantly improved.</p>
</div>
</div>
</div>
</div>
<div id="summary-of-learning-and-feedback-networks-draft" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Summary of Learning and Feedback Networks [DRAFT]</h3>
<p>Based on our previous discussion, we find that both QG and QA (including VQA) tasks have beem well-researched. A numerous of spcifically designed algorithms were presented and proved effective for solving these problems.</p>
<p>There are also some research has been done to tackle both QG and QA problems at the same time. The research we presented may not be particularly applicable to the video content. But the implication is clear, by combining some techniques we learned from the QG and QA sections separtely with some frameworks like dual learning introduced in the dual task section, it is plausible for us to conclude that the coherent QG + QA system for our purpose is not that far-fetched.</p>
<p>As what we anticipated, the research is significantly less in the open-ended question realm. However, some techniques presented in our research finding session are very relevant and could certainly serve as our starting point for solving our problem such as AES model and open-ended question answering networks. As more and more research comes out, we should expect more effective solutions to come out soon.</p>
<p>Current reserach is promising but we need more reserach and innovation in this area.</p>
<p>In open-ended question section, we find that our current research cannot fully support our goal of taking in the answer in any possible formats (i.e. a short video presentation or a recording) besides writing.</p>
<div id="datasets-and-annotation-needed" class="section level4">
<h4><span class="header-section-number">4.2.5.1</span> Datasets and Annotation Needed</h4>
<p>In order to approach this problem from scratch, we need to create our own dataset for which we will provide some related resources to start with:</p>
<ol style="list-style-type: decimal">
<li><a href="https://research.google.com/youtube8m/">YouTube-8M Dataset</a>. This is a large-scale labeled video dataset that consists of millions of YouTube video IDs, with high-quality generated annotations from a diverse vocabulary of 3800+ visual entities. As you can see from its introduction, it comes with precomputed audio-visual features from billions of frames and audio segments. In short, we can expect the following content from this dataset:</li>
</ol>
<ul>
<li><p>the dataset consists of 6.1M videos URLs, labeled with a vocabulary of 3863 visual entities</p></li>
<li><p>the video-level dataset comes out to be 18 GB in size, whiel the frame-level features are approximately 1.3 TB</p></li>
<li><p>it comes with pre-extracted audio &amp; visual features from every second of video.</p></li>
</ul>
<p>Though the video content is not limited to education category, we can still use it to get a strong baseline model.</p>
<p>Naturally, the next step would be constrain our model to train on particularly educational content. The data needed for taining may include the raw video clip, annotation/caption of the whole video content, and audio part of the video.</p>
<ol start="2" style="list-style-type: decimal">
<li>In this study <a href="http://jofdl.nz/index.php/JOFDL/article/download/255/198">Video Captions for Online Courses: Do YouTube’s Auto-generated Captions Meet Deaf Students’ Needs?</a>, they studied the auto-generated captions generated on YouTube online courses. They find that, on average, there were 7.7 phrase errors per minute of a total 68 minutes video caption. It is been said, we still need to do a lot of annotion work before we can finally compose our own training dataset.</li>
</ol>
<p>Other resources that could possibly help us with out tasks are as below:</p>
<ol start="3" style="list-style-type: decimal">
<li><a href="http://videomcc.org/">VideoMCC</a>. In their paper, they formulate Video Multiple Choice Caption (VideoMCC) as a way to assess video comprehension through an easy-to-interpret performance measure. In their paper <a href="https://arxiv.org/pdf/1606.07373.pdf">VideoMCC: a New Benchmark for Video Comprehension</a> they propose to cast video understanding in the form of multiple choice tests that assess the ability of the algorithm to comprehend the semantics of the video. Example is as below:</li>
</ol>
<div class="figure">
<img src="img/mcc.png" alt="VideoMCC" />
<p class="caption">VideoMCC</p>
</div>
<ol start="4" style="list-style-type: decimal">
<li>As what we have covered earlier, this paper <a href="https://arxiv.org/pdf/1512.02902.pdf">MovieQA: Understanding Stories in Movies through Question-Answering</a>, they introduced a new dataset called MovieQA dataset that can evaluate automatic story comprehension from both video and text.</li>
</ol>
<p>Here are 2 figures that can help you better understand their dataset:</p>
<div class="figure">
<img src="img/movieqa1.png" alt="MovieQA_1" />
<p class="caption">MovieQA_1</p>
</div>
<div class="figure">
<img src="img/movieqa2.png" alt="MovieQA_2" />
<p class="caption">MovieQA_2</p>
</div>
<ol start="5" style="list-style-type: decimal">
<li><p>In this paper <a href="https://arxiv.org/pdf/1806.00186.pdf">Video Description: A Survey of Methods, Datasets and Evaluation Metrics</a> mulple methods, datasets and evluation metrics for video description task in a comprehensive survey.</p></li>
<li><p>Inspired by this paper <a href="https://arxiv.org/pdf/1808.07036.pdf">QuAC : Question Answering in Context</a> in which they present QuAC dataset for QA in Context that contains 14K information-seeking QA dialogs such as a student who poses a sequence of freeform question to learn as much as possible about a hidden Wikipedia text or a teacher who answers the questions by providing short excerpts from the text, we are convinced that it is might also be possibleob to develop a system that can allow student to pause the video and ask our system a information-seeking question and then get the answer from our system based on the curent content. .</p></li>
</ol>
</div>
</div>
</div>
<div id="knowledge-graph-draft" class="section level2">
<h2><span class="header-section-number">4.3</span> Knowledge Graph [DRAFT]</h2>
<p>Next, we need to consider how we can select an adequate and relevant learning masterial and generate an effecive learning map for the learners based on their current progress and the general knowledge graph/map, given the ever growing amount of educational content on the web.</p>
<p>As I mentioned earlier, learning is a knowledge accumulation process. Knowledge itself has its unique structure that can help us learn in a most effective and productive way. Knowledge Graph is a great tool that we developed to map and present the structure of knolwedge. In shirt, knowledge graphs are collections of relational facts, where each fact states that a certrain relation holds between 2 entities.</p>
<p>Now we will consider the knowledge graph as the backbone.</p>
<div id="what-is-a-graph" class="section level3">
<h3><span class="header-section-number">4.3.1</span> What is a graph?</h3>
<blockquote>
<p>Graphs are networks of dots and lines - Graph Theory (Dover Books)</p>
</blockquote>
<p>Mathematically speaking, graphs are mathematical structures used to model pairwise relations between objects. A graph in this context is made of vertices, nodes, or points which are connected by edges, arcs or lines. Typically a graph consists of two sets. A set of vertexes and a set of edges <span class="math display">\[GRAPH_{v,e} =
 \begin{pmatrix}
  v_{1,1} &amp; a_{1,2} &amp; \cdots &amp; a_{1,n} \\
  e_{2,1} &amp; e_{2,2} &amp; \cdots &amp; e_{2,n} \\
 \end{pmatrix}\]</span></p>
<p>As for the knowledge graph, it is a knowledge base. Often when people talk about knowledge graph, they are refering to the multi-relational graph used by Google and its services to enhance search engine’s results with information gathered from a variety of sources. Per Wikipeida, <a href="https://developers.google.com/knowledge-graph/#knowledge_graph_entities">Google’s Knowledge Graph</a> uses a graph database to provide structured and detailed information about the topic in addition to a list of links to other sites.</p>
<p>In general, a knowledge graph represents a knowledge domain. It connects things of different types in a systematic way. Knowledge graphs encode knowledge arranged in a network of nodes and links rather than tables and columns. With knolwedge graphs, people and machines can benefit from a dynamically growing semantic network of facts about things. In other wrods, we can use it to capture the facts related to people, processes, applications, data and many other custom ojbects as well as their relationships among them.</p>
<p>If you want, they can also capture evidence that can be used to attribute the strengths of these relationships.</p>
<p>Also, we have found a lot of applications that demonstrate that existing generic knowledge graphs have shown advantages to support semantic search(i.e. Google’s Knowledge Graph), personal assistant(i.e. Apple’s Siri) and deep question answering (i.e. Wolfram Alpha and IBM’s Watson).</p>
</div>
<div id="problem-formulation-2" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Problem Formulation</h3>
<p>Given we’ve impletemented the learning and feedback module, knowing where a given piece of education content fits into the knowledge space is a vital task if we want the knowledge footprint to make a learner predictable to others as well as being able to recommend new educational content that the learner can take on successfully. We will need the following things to connect educational content.</p>
<blockquote>
<p>A classifier to take a piece of educational content</p>
</blockquote>
<p>A graph dedicated to education should do the following:</p>
<ul>
<li><p>Provide flexibility to add new subjects</p></li>
<li><p>Connect related subjects to each</p></li>
<li><p>Map concepts with the subject</p></li>
<li><p>Connect concepts related to the content</p></li>
</ul>
<div class="figure">
<img src="img/knowledgeEcosystem.png" alt="Knowledge Ecosystem" />
<p class="caption">Knowledge Ecosystem</p>
</div>
</div>
<div id="automatic-knowledge-graph-construction" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Automatic Knowledge Graph Construction</h3>
<p>Classic knowledge representation techniques allow a knowledge engineer o create rules that can be interpreted by a reasoner to infer new or missing triples(subject, predicate, object). These rules are usually expressed through an ontology which allows for the propagation of properties from top classes to the lower classes.</p>
<p>However, we are looking for solutions that can allow us to complete our educational knowledge graph building process. Based on our research, generic knowledge graphs uaually cannot sufficiently support many domain-specific applications i.e. education and finding the representation of the graph to feed the triples into a machine learning or deep learning algorithm is still an open area of research. As a start, let’s focus on how to automate the eduction knowledge graph construction process.</p>
<p>There have been several papers that provide promising results to the automatation of constructing a knowledge graph. Let’s take a look.</p>
</div>
<div id="case-studies-5" class="section level3">
<h3><span class="header-section-number">4.3.4</span> Case Studies</h3>
<ol style="list-style-type: decimal">
<li>In this paper <a href="https://ieeexplore.ieee.org/document/8362657">KnowEdu: A System to Construct Knowledge Graph for Education</a>, they propose a system KnowEdu that can automatically construct knowledge graph for education. In short, the system is able to extract concepts of subjects or courses and then identifies the educational relations between the concepts.</li>
</ol>
<p>More importantly, it adopts the neural sequence labeling algorithm on pedagogical data to extract instruction concepts and employs probabilistic association rule mining on learning assessment data to identify the significance of the relations.</p>
<p>In sum, their system consists of the following modules:</p>
<ul>
<li><p>Instructional Concept Extraction Module to extract instructional concepts for a given subject or course.</p></li>
<li><p>Educational Relation Identification Module to identify the educational relations that interlink instructional concepts to assist the learning and teaching process directly.</p></li>
</ul>
<p>Below is a block diagram of KnowEdu System.</p>
<div class="figure">
<img src="img/knowedu.png" alt="knowedu" />
<p class="caption">knowedu</p>
</div>
<p>They used conditional random field (CRF) model for entity or terminology recognition task. Moreover, they adopt neural network, or more particularly Gated recurrent unit network (GRU) architecture for neural sequence labeling on educational entity extraction task.</p>
<p>In terms of relation identification they implement probabilistic association data mining techniques on learning assessment adata and accomplish the task of educational relation identification.</p>
<p>A snapshot of the knowledge graph for mathematics generated by knowedu system.</p>
<div class="figure">
<img src="img/kg.png" alt="knowledge_graph" />
<p class="caption">knowledge_graph</p>
</div>
<p>Below are some approaches related to knowledge graph (KG) embedding which is used to embed components of a KG including entities and relations into continuous vector space so as to simply the manipulation while preserving the inherent structure of a KG.</p>
<p>As for its benefits and importance for our task, it can help with a variety of downstream tasks i.e. KG completion and relation extraction, and hence be used to drasically improve the infomration acquisition speed for KG.</p>
<ol start="2" style="list-style-type: decimal">
<li>In this paper <a href="http://www.mlgworkshop.org/2018/papers/MLG2018_paper_5.pdf">Generalized Embedding Model for Knowledge Graph Mining</a>, they have presented a model for learning neural presentation of generalized knowledge graphs using a novel muli-shot unsupervised neural network model, called the <strong>Graph Embedding Network (GEN)</strong>. This model is able to learn different types of knowlege graphs from a universal perspective and it provides flexibility in learning representations that work on graphs conforming to different domains.</li>
</ol>
<p>In developint their model, they extend the traditional one-shot supervised learning meachnism by introducing a multi-shot unsupervised learning framework where a 2-layer MLP network for every shot. This framework can in turn be used to accommodate both homogeneous and heterogeneous networks.</p>
<ol start="3" style="list-style-type: decimal">
<li>In this paper <a href="https://openreview.net/pdf?id=rJ4qXnCqFX">Probabilisic Knowledge Graph Embeddings</a>, they explored a new type of embedding model that can link prediction in relational knowledge graph. They start from a problem that even large knowledge graphs typically contain only few facts per entity, leading effectively to a small data problem where parameter uncertainty matters. As for the solution, they suggest that the knolwedge graphs should be treated within a Bayesian framework.</li>
</ol>
<p>In short, they present a probabilistic interpretation of existing knowledge graph embedding models. By reformulating the models like ComplEx and DistMult, they construct the generative models for relational facts.</p>
<p>They also apply stochastic variational inference to scalably estimate an approximate posterior for each entity and relation embedding in the knowledge graph. By doing so, they can estimate the uncertainty, but more importantly, they can use gradient-based hyperparmeter optimization by stochastic gradient descent on the optimized variational bound.</p>
<p>As a result, their model shows experimentally new state-of-art results in link prediction task.</p>
</div>
<div id="summary-of-knowledge-graph-draft" class="section level3">
<h3><span class="header-section-number">4.3.5</span> Summary of Knowledge Graph [DRAFT]</h3>
<p>Based on our research, we are very pleased to see some great progress has been made to bridge the KG automation process with deep learning and other machine learning techniques.</p>
<p>As what we discussed above, the first paper introduces a system that almost exactly matches our goal. The carefully walk us through the current progress and possible solutions for solving each obstacles in developing such a educational knowledge graph. For the instructional concept extraction task, they use both CRF model and neural sequence labeling algorithm to achieve a high performance. They employ probabilistic association rule mining on learning assessment data to identify the relations with educational significance.</p>
<p>The last 2 papers demonstrate the progress that has been made in KG embedding learning domain. As mentioned above, as one of the most effective methods in representing knowledge graphs. The development of this field may offer some great implication for the future KG automation and acquisition work/research.</p>
<div id="key-components" class="section level4">
<h4><span class="header-section-number">4.3.5.1</span> Key Components</h4>
<p>Here are some key components that we found important of building an automated knowledge graph for education:</p>
<ol style="list-style-type: decimal">
<li><p>Entity recognition that aims to extract concept of interest from structured or unstructured data.</p></li>
<li><p>Relation identification that leverages on the semantic meaning of data.</p></li>
</ol>
</div>
<div id="possible-next-steps" class="section level4">
<h4><span class="header-section-number">4.3.5.2</span> Possible Next Steps</h4>
<p>Here are some of our research summary regarding the steps of creating such an educational knowledge graph:</p>
<ol style="list-style-type: decimal">
<li><p>In terms of entity recognition task, We need to first get the data from some reliable open sementic sources i.e. Wikipedia or Freebase. Or we can crawl the data online on our own to find high quality training data. In the next section we will be listing out some resources that might help.</p></li>
<li><p>Next, we need to create the model to map the relation among the entities. There are plenty of great github that we can use to help us with this task. Or we can use some tools developed for this purpose i.e. node.js and Wolfram Mathematica embedded symbolic functions or just Tensorflow. The typically techniques we will employ are NLP and semantic data tagging or labeling techniques.</p></li>
<li><p>Naturally, after the entity extraction and relationship mapping we will visualize our map.</p></li>
</ol>
</div>
<div id="datasets-and-annotaters-needed" class="section level4">
<h4><span class="header-section-number">4.3.5.3</span> Datasets and Annotaters needed</h4>
<ol style="list-style-type: decimal">
<li><a href="https://ai.google/research/pubs/pub45634">Knowledge Vault</a>: A web-scale approach to probabilistic knowledge fusion. In this paper <a href="https://dejanseo.com.au/wp-content/uploads/2014/08/Knowledge-Vault-A-Web-Scale-Approach-to-Probabilistic-Knowledge-Fusion.pdf">Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion</a>, they introduce Knowledge Vault that combines extraction from Web content (obained through analysis of text, tabular data, page structure, and human annotation) with prior knowledge derived from existing knowledge repositories.</li>
</ol>
<p>They employ a supervised machine learning models for fusing these distinct information sources. As a result, their system can automatically construct a web-scale probabilistic knowledge base.</p>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://developers.google.com/knowledge-graph/#knowledge_graph_entities">Google Knowledge graph Search API</a>.</li>
</ol>
<p>Below are some resources that might be helpful for starting such a job from scrath:</p>
<ol start="3" style="list-style-type: decimal">
<li><p><a href="https://developers.google.com/search/docs/guides/intro-structured-data">Understand how structured data works</a> by Google.</p></li>
<li><p><a href="wikipedia.com">Wikipedia</a></p></li>
<li><p><a href="Freebase.com">Freebase</a></p></li>
</ol>
</div>
</div>
</div>
<div id="knowledge-journeys-draft" class="section level2">
<h2><span class="header-section-number">4.4</span> Knowledge Journeys [DRAFT]</h2>
<p>The knowledge graph is our ground truth and can be applied universally to some extent, but everyone’s learning journey is still highly custom. In terms of learning, everyone seems to have their unique set of problems that they are curious about and everyone is on their own mission towards the mastery. As a result, their knowlege journeys could have a lot more degree of freedom depends on the learn’s learning history, interests and who they are related to.</p>
<p>We cannot possibly put such an online learning/teaching system into use without taking this crucial factor into our account. However, this is not a trivial problem that can be solved with 1 network or 2.</p>
<p>Let’s first formulate our problem before we dive into the possible solutions. Below are few key components that we need to combine to achieve our ultimate goal which is to appropriately guide the learner through their unique knowledge journey:</p>
<ol style="list-style-type: decimal">
<li>First, we need to</li>
</ol>
<p>, we can rely on some heuristic and models that have been developed to resolve this type of idiosymcratic issue.</p>
<p>As we all know that a recommender system is an intuitive line of defense against consumer over-choice given the evern growing information available on the web. As we mentioned earlier in the knowledge graph, a authoritative and personalized recommending system is essential for facilitating the learning.</p>
<p>Typically, a recommendation models can be classified into 3 main categories:</p>
<ol style="list-style-type: decimal">
<li><p>Collaborative filtering</p></li>
<li><p>Content based</p></li>
<li><p>Hybrid recommender system</p></li>
</ol>
<p>As I mentioned, here we will mainly focus on hybrid recommender system.</p>
<p>There are a diverse array of achitectual paradigms that are closely related recommending system. Let’s take a look at few of them: 1. Autoencoder</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Convolutional Neural Network</p></li>
<li><p>Recurrent Neural Network</p></li>
<li><p>Restricted Boltzmann Machine (RBM)</p></li>
<li><p>Adversarial Networks</p></li>
<li><p>Attentional Models (AM)</p></li>
<li><p>Deep Reinforcement Learning (DRL)</p></li>
</ol>
<div id="summary-of-current-research-and-needs-draft" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Summary of Current Research and Needs [DRAFT]</h3>
</div>
</div>
<div id="data-and-annotation-draft" class="section level2">
<h2><span class="header-section-number">4.5</span> Data and Annotation [DRAFT]</h2>
<div id="reference-draft" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Reference [DRAFT]</h3>
<ol style="list-style-type: decimal">
<li><p><a href="https://arxiv.org/pdf/1808.04961.pdf">A Framework for Automatic Question Generation from Text using Deep Reinforcement Learning</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1705.00106.pdf">Learning to Ask: Neural Question Generation for Reading Comprehension</a></p></li>
<li><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yalong_Bai_Deep_Attention_Neural_ECCV_2018_paper.pdf">Deep Attention Neural Tensor Network for Visual Question Answering</a></p></li>
<li><p><a href="http://www.cs.cornell.edu/~xdu/papers/acl17_dsc_poster.pdf">Learning to Ask</a></p></li>
<li><p><a href="https://openreview.net/pdf?id=rk3pnae0b">TOPIC-BASED QUESTION GENERATION</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1707.07435.pdf">Deep Learning based Recommender System</a></p></li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="concepts.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conclusion.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dyadxmachina/can-machines-teach/edit/master/02-survey.Rmd",
"text": "Edit"
},
"download": ["A Knowledge Ecosystem - Deep Learning and Education.pdf", "A Knowledge Ecosystem - Deep Learning and Education.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
